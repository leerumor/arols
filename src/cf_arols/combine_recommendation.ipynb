{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from numpy import *\n",
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# read files\n",
    "def load_files(method):\n",
    "    train_data = np.load(path + method + '_train_data.npy').item()\n",
    "    test_data = np.load(path + 'user_test_data.npy').item()\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item-CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_distance_matrix(name):\n",
    "    matrix = pd.read_csv(path + name + '.csv')\n",
    "    return matrix.set_index('Unnamed: 0')\n",
    "\n",
    "class ItemCF:\n",
    "    def __init__(self, metric='jaccard'):\n",
    "        self.metric = metric\n",
    "    \n",
    "    def fit(self, cluster, train_data):\n",
    "        self.train_data = train_data\n",
    "        #self.distance_matrix = generate_distance_matrix(self.train_data, self.metric)\n",
    "        self.distance_matrix = get_distance_matrix('item_' + self.metric + '_matrix_' + str(cluster))\n",
    "        return self.distance_matrix\n",
    "    \n",
    "    def predict(self, user_id, k=10):\n",
    "        predictions = {}\n",
    "        if user_id not in self.train_data.keys(): return {}\n",
    "        for item, value in self.train_data[user_id].items():\n",
    "            neighbours = self.distance_matrix[str(item)].drop(item).sort_values(ascending=False)[:k]\n",
    "            neighbours = neighbours.fillna(min(neighbours))\n",
    "            #total_dist = sum(neighbours)\n",
    "            #if total_dist == 0: total_dist = 1\n",
    "            #neighbours = neighbours.apply(lambda x: x/total_dist) #求用户权重，越近权重越大\n",
    "            for neighbour in neighbours.index:\n",
    "                if neighbour not in self.train_data[user_id].keys():\n",
    "                    predictions.setdefault(item, 0)\n",
    "                    predictions[item] += neighbours[neighbour]*value\n",
    "        return dict(sorted(predictions.items(), key=lambda e: e[1], reverse=True)[:k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "def dict2list(data):\n",
    "    listtmp = []\n",
    "    for subDict in data.values():\n",
    "        listtmp.append(list(subDict.keys()))\n",
    "    return listtmp\n",
    "\n",
    "def loadDataSet():\n",
    "    return [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]\n",
    "\n",
    "def createC1(dataSet):\n",
    "    C1 = []\n",
    "    for transaction in dataSet:\n",
    "        for item in transaction:\n",
    "            if not [item] in C1:\n",
    "                C1.append([item])\n",
    "                \n",
    "    C1.sort()\n",
    "    return list(map(frozenset, C1))#use frozen set so we\n",
    "                            #can use it as a key in a dict    \n",
    "\n",
    "def scanD(D, Ck, minSupport):\n",
    "    ssCnt = {}\n",
    "    for tid in D:\n",
    "        for can in Ck:\n",
    "            if can.issubset(tid):\n",
    "                ssCnt.setdefault(can, 0)\n",
    "                ssCnt[can] += 1\n",
    "    numItems = float(len(D))\n",
    "    retList = []\n",
    "    supportData = {}\n",
    "    for key in ssCnt:\n",
    "        support = ssCnt[key]/numItems\n",
    "        if support >= minSupport:\n",
    "            retList.insert(0,key)\n",
    "        supportData[key] = support\n",
    "    return retList, supportData\n",
    "\n",
    "def aprioriGen(Lk, k): #creates Ck\n",
    "    retList = []\n",
    "    lenLk = len(Lk)\n",
    "    for i in range(lenLk):\n",
    "        for j in range(i+1, lenLk): \n",
    "            L1 = list(Lk[i])[:k-2]; L2 = list(Lk[j])[:k-2]\n",
    "            L1.sort(); L2.sort()\n",
    "            if L1==L2: #if first k-2 elements are equal\n",
    "                retList.append(Lk[i] | Lk[j]) #set union\n",
    "    return retList\n",
    "\n",
    "def apriori(dataSet, minSupport = 0.5):\n",
    "    C1 = createC1(dataSet)\n",
    "    D = list(map(set, dataSet))\n",
    "    L1, supportData = scanD(D, C1, minSupport)\n",
    "    L = [L1]\n",
    "    k = 2\n",
    "    while (len(L[k-2]) > 0):\n",
    "        Ck = aprioriGen(L[k-2], k)\n",
    "        Lk, supK = scanD(D, Ck, minSupport)#scan DB to get Lk\n",
    "        supportData.update(supK)\n",
    "        L.append(Lk)\n",
    "        k += 1\n",
    "    return L, supportData\n",
    "\n",
    "def generateRules(L, supportData, minConf=0.7, minKulc=0.7, maxIR=0.5):  #supportData is a dict coming from scanD\n",
    "    bigRuleList = []\n",
    "    for i in range(1, len(L)):#only get the sets with two or more items\n",
    "        for freqSet in L[i]:\n",
    "            H1 = [frozenset([item]) for item in freqSet]\n",
    "            if (i > 1):\n",
    "                rulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf)\n",
    "            else:\n",
    "                calcConf(freqSet, H1, supportData, bigRuleList, minConf, minKulc, maxIR)\n",
    "    return bigRuleList         \n",
    "\n",
    "def calcConf(freqSet, H, supportData, brl, minConf=0.7, minKulc=0.7, maxIR=0.5):\n",
    "    prunedH = [] #create new list to return\n",
    "    for conseq in H:\n",
    "        if freqSet-conseq not in supportData or conseq not in supportData: continue\n",
    "        conf = supportData[freqSet]/supportData[freqSet-conseq] #calc confidence\n",
    "        kulc = (conf+ supportData[freqSet]/supportData[conseq])/2\n",
    "        imbalance = abs(supportData[freqSet-conseq]-supportData[conseq])/(supportData[freqSet-conseq]+supportData[conseq]-supportData[freqSet])\n",
    "        if conf >= minConf and kulc>=minKulc and imbalance<=maxIR: \n",
    "            #print(freqSet-conseq,'-->',conseq,'conf:',conf,'kulc:', kulc, 'imbalance:', imbalance)\n",
    "            brl.append((freqSet-conseq, conseq, conf, kulc, imbalance))\n",
    "            prunedH.append(conseq)\n",
    "    return prunedH\n",
    "\n",
    "def rulesFromConseq(freqSet, H, supportData, brl, minConf=0.7):\n",
    "    m = len(H[0])\n",
    "    if (len(freqSet) > (m + 1)): #try further merging\n",
    "        Hmp1 = aprioriGen(H, m+1)#create Hm+1 new candidates\n",
    "        Hmp1 = calcConf(freqSet, Hmp1, supportData, brl, minConf)\n",
    "        if (len(Hmp1) > 1):    #need at least two sets to merge\n",
    "            rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)\n",
    "            \n",
    "def pntRules(ruleList, itemMeaning):\n",
    "    for ruleTup in ruleList:\n",
    "        for item in ruleTup[0]:\n",
    "            print(itemMeaning[item])\n",
    "        print(\"           -------->\")\n",
    "        for item in ruleTup[1]:\n",
    "            print(itemMeaning[item])\n",
    "        print(\"confidence: %f\" % ruleTup[2])\n",
    "        print(' ')      #print a blank line\n",
    "\n",
    "def mine_rules(train_data, minSup=0.15, minConf=0.9, minKulc=0.7, maxIR=0.5):\n",
    "    train_list = dict2list(train_data)\n",
    "    freq_items, support = apriori(train_list, minSupport=minSup) # freq_items is a 2D list, support is a dict\n",
    "    ass_rules = generateRules(freq_items, support, minConf, minKulc, maxIR) # ass_rules is a list of set (setA, setB, conf)\n",
    "    return freq_items, support, ass_rules\n",
    "\n",
    "def filter_rules_by_conf(ass_rules, minConf):\n",
    "    new_list = list()\n",
    "    for left, rightSet, conf, kulc, ir in ass_rules:\n",
    "        if conf>=minConf:\n",
    "            new_list.append((left, rightSet, conf, kulc, ir))\n",
    "    return new_list\n",
    "\n",
    "def filter_rules_by_kulc(ass_rules, minKulc):\n",
    "    new_list = list()\n",
    "    for left, rightSet, conf, kulc, ir in ass_rules:\n",
    "        if kulc>=minKulc:\n",
    "            new_list.append((left, rightSet, conf, kulc, ir))\n",
    "    return new_list\n",
    "\n",
    "def filter_rules_by_IR(ass_rules, maxIR):\n",
    "    new_list = list()\n",
    "    for left, rightSet, conf, kulc, ir in ass_rules:\n",
    "        if ir<=maxIR:\n",
    "            new_list.append((left, rightSet, conf, kulc, ir))\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CF-AROLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recommendOnCombinationV1(factor, N, algo, rules, support, trainDict, testData, th=0.7):\n",
    "    hitCF = 0\n",
    "    #hitRule = 0\n",
    "    hitComb = 0\n",
    "    allR = 0\n",
    "    allPCF = 0\n",
    "    #allPRule = 0\n",
    "    allPComb = 0\n",
    "    recommendDictCF = {}\n",
    "    #recommendDictRule = {}  # 推荐集合-分数\n",
    "    recommendDictComb = {}\n",
    "    #recommendSetRule = set()\n",
    "    #recommendSetComb = set()\n",
    "    students = int(len(testData)*0.1)\n",
    "    for user, itemdict in testData.items():\n",
    "        if user not in trainDict.keys(): continue\n",
    "        if np.random.random()>0.1: continue\n",
    "        if students == 0: break\n",
    "        students -= 1\n",
    "        items = itemdict.keys()\n",
    "        recommendDictCF.clear()\n",
    "        recommendDictCF = recommender.predict(user, k=factor*N) #多返回一些推荐item\n",
    "        for recItem, score in list(recommendDictCF.items())[:N]:\n",
    "            if recItem in items:\n",
    "                hitCF += 1\n",
    "\n",
    "        allR += len(items)\n",
    "        allPCF += N\n",
    "\n",
    "        avgCF = 1\n",
    "        if not len(recommendDictCF.values()) == 0:\n",
    "            maxCF = max(recommendDictCF.values())\n",
    "            minCF = min(recommendDictCF.values())\n",
    "            avgCF = (maxCF+minCF)/2\n",
    "\n",
    "        #Association Rules\n",
    "        #recommendDictRule.clear()\n",
    "        recommendDictComb.clear()\n",
    "        #recommendSetRule.clear()\n",
    "        #recommendSetComb.clear()\n",
    "        history = trainDict[user].keys()\n",
    "        # 遍历规则，添加推荐\n",
    "        for left, rightSet, conf, kulc, ir in rules:\n",
    "            if set(left).issubset(history):\n",
    "                if not set(rightSet).issubset(history) and rightSet in support:\n",
    "                    #recommendDictRule.setdefault(rightSet, 0)\n",
    "                    recommendDictComb.setdefault(rightSet, 0)\n",
    "                    #recommendDictRule[rightSet] += conf #* support[rightSet]\n",
    "                    recommendDictComb[rightSet] += conf #* support[rightSet]\n",
    "\n",
    "\n",
    "        #for item, score in sorted(recommendDictRule.items(), key=lambda x: x[1], reverse=True):\n",
    "        #    recommendSetRule = recommendSetRule | set(item)\n",
    "        #    if len(recommendSetRule) >= N: break\n",
    "\n",
    "        #hitRule += len(recommendSetRule & set(items))\n",
    "        #allPRule += len(recommendSetRule)\n",
    "\n",
    "        #混合推荐算法：\n",
    "        avgComb = 1\n",
    "        if not len(recommendDictComb.values()) == 0:\n",
    "            maxComb = max(recommendDictComb.values())\n",
    "            minComb = min(recommendDictComb.values())\n",
    "            avgComb = (maxComb+minComb)/2\n",
    "\n",
    "        tmp = {}\n",
    "\n",
    "        for itemSet, score in recommendDictComb.items():\n",
    "            for item in itemSet:\n",
    "                tmp.setdefault(item, 0)\n",
    "                if item in recommendDictCF.keys():\n",
    "                    tmp[item] += score*recommendDictCF[item]\n",
    "                else:\n",
    "                    tmp[item] += score*avgCF\n",
    "                    \n",
    "        for item, score in recommendDictCF.items():\n",
    "                tmp.setdefault(item,0)\n",
    "                tmp[item] += score*avgComb\n",
    "\n",
    "        threshold = avgComb*avgCF*th\n",
    "        #归一化recommendDictCF的值并使得recommendDictRule乘处理后的值，没有的乘平均值（毕竟CF准确率低）\n",
    "        for item, score in sorted(tmp.items(), key=lambda x: x[1], reverse=True)[:N]:\n",
    "            if(score < threshold): continue\n",
    "            allPComb += 1\n",
    "            if item in items:\n",
    "                hitComb += 1\n",
    "\n",
    "    precisionCF = hitCF * 100 / (allPCF * 1.0)\n",
    "    recallCF = hitCF * 100 / (allR * 1.0)\n",
    "    precisionComb = hitComb * 100 / (allPComb * 1.0)\n",
    "    recallComb = hitComb * 100 / (allR * 1.0)\n",
    "\n",
    "    return precisionCF, recallCF, precisionComb, recallComb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster 0\n",
      "param 0\n",
      "param 10\n",
      "param 20\n",
      "param 30\n",
      "param 40\n",
      "param 50\n",
      "param 60\n",
      "param 70\n",
      "param 80\n",
      "param 90\n",
      "param 100\n",
      "cluster 1\n",
      "param 0\n",
      "param 10\n",
      "param 20\n",
      "param 30\n",
      "param 40\n",
      "param 50\n",
      "param 60\n",
      "param 70\n",
      "param 80\n",
      "param 90\n",
      "param 100\n",
      "cluster 2\n",
      "cluster 3\n",
      "param 0\n",
      "param 10\n",
      "param 20\n",
      "param 30\n",
      "param 40\n",
      "param 50\n",
      "param 60\n",
      "param 70\n",
      "param 80\n",
      "param 90\n",
      "param 100\n",
      "cluster 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-bb595eec183f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mdistance_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecommender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mfreq_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mass_rules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmine_rules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminSup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminSup_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminConf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.84\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminKulc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.88\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIR\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-6515dbdf5b85>\u001b[0m in \u001b[0;36mmine_rules\u001b[0;34m(train_data, minSup, minConf, minKulc, maxIR)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mtrain_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict2list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mfreq_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapriori\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminSupport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminSup\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# freq_items is a 2D list, support is a dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mass_rules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerateRules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminConf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminKulc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIR\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ass_rules is a list of set (setA, setB, conf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfreq_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mass_rules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-6515dbdf5b85>\u001b[0m in \u001b[0;36mgenerateRules\u001b[0;34m(L, supportData, minConf, minKulc, maxIR)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mH1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfrozenset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfreqSet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mrulesFromConseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqSet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupportData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigRuleList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mcalcConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqSet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupportData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigRuleList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminConf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminKulc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-6515dbdf5b85>\u001b[0m in \u001b[0;36mrulesFromConseq\u001b[0;34m(freqSet, H, supportData, brl, minConf)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqSet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#try further merging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mHmp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maprioriGen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#create Hm+1 new candidates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mHmp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalcConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqSet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHmp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupportData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHmp1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m#need at least two sets to merge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mrulesFromConseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqSet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHmp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupportData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-6515dbdf5b85>\u001b[0m in \u001b[0;36mcalcConf\u001b[0;34m(freqSet, H, supportData, brl, minConf, minKulc, maxIR)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mminConf\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkulc\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0mminKulc\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mimbalance\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0mmaxIR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;31m#print(freqSet-conseq,'-->',conseq,'conf:',conf,'kulc:', kulc, 'imbalance:', imbalance)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mbrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqSet\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mconseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkulc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimbalance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mprunedH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprunedH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "path='./birch6_final/'\n",
    "\n",
    "cluster=0\n",
    "N=10\n",
    "minSup_list = [0.13, 0.11, 0, 0.13, 0.17, 0.21]\n",
    "param_name = 'th'\n",
    "param_list = range(0, 110, 10)\n",
    "\n",
    "results = pd.DataFrame(columns=['cluster', 'param', 'n_rules', 'precisionCF', 'recallCF', 'f1scoreCF', 'precisionComb', 'recallComb', 'f1scoreComb'])\n",
    "train_data, test_data = load_files('user')\n",
    "\n",
    "for cluster in range(0, 6):\n",
    "    print('cluster', cluster)\n",
    "    \n",
    "    if cluster == 2: continue\n",
    "    recommender = ItemCF(metric='cosine')\n",
    "    distance_matrix = recommender.fit(cluster, train_data[cluster])\n",
    "\n",
    "    freq_items, support, ass_rules = mine_rules(train_data[cluster], minSup=minSup_list[cluster], minConf=0.84, minKulc=0.88, maxIR=0.15)\n",
    "    \n",
    "    for param in param_list:\n",
    "        print('param', param)\n",
    "        minParam = float(param)/100\n",
    "        new_ass_rules = ass_rules\n",
    "        #if param<50: new_ass_rules = filter_rules_by_IR(ass_rules, minParam)\n",
    "        n_rules = len(new_ass_rules)\n",
    "        precisionCF, recallCF, precisionComb, recallComb = recommendOnCombinationV1(2, N, recommender, new_ass_rules, support, \n",
    "                                                                            train_data[cluster], test_data[cluster], th=minParam)\n",
    "        results.loc[len(results)] = [cluster, float(param)/100, n_rules, precisionCF, recallCF, 2*precisionCF*recallCF/(precisionCF+recallCF),precisionComb, recallComb, 2*precisionComb*recallComb/(precisionComb+recallComb)]\n",
    "\n",
    "results.to_csv(path+'birch_cf_arols_'+param_name+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster 0\n",
      "param 5\n",
      "param 10\n",
      "param 15\n",
      "param 20\n",
      "param 25\n",
      "param 30\n",
      "param 35\n",
      "param 40\n",
      "param 45\n",
      "param 50\n",
      "param 55\n",
      "param 60\n",
      "cluster 1\n",
      "param 5\n",
      "param 10\n",
      "param 15\n",
      "param 20\n",
      "param 25\n",
      "param 30\n",
      "param 35\n",
      "param 40\n",
      "param 45\n",
      "param 50\n",
      "param 55\n",
      "param 60\n",
      "cluster 2\n",
      "cluster 3\n",
      "param 5\n",
      "param 10\n",
      "param 15\n",
      "param 20\n",
      "param 25\n",
      "param 30\n",
      "param 35\n",
      "param 40\n",
      "param 45\n",
      "param 50\n",
      "param 55\n",
      "param 60\n",
      "cluster 4\n",
      "param 5\n",
      "param 10\n",
      "param 15\n",
      "param 20\n",
      "param 25\n",
      "param 30\n",
      "param 35\n",
      "param 40\n",
      "param 45\n",
      "param 50\n",
      "param 55\n",
      "param 60\n",
      "cluster 5\n",
      "param 5\n",
      "param 10\n",
      "param 15\n",
      "param 20\n",
      "param 25\n",
      "param 30\n",
      "param 35\n",
      "param 40\n",
      "param 45\n",
      "param 50\n",
      "param 55\n",
      "param 60\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "path='./birch6_validate/'\n",
    "\n",
    "cluster=0\n",
    "N=10\n",
    "minSup_list = [0.13, 0.11, 0, 0.13, 0.17, 0.21]\n",
    "param_name = 'N'\n",
    "param_list = range(5, 65, 5)\n",
    "\n",
    "results = pd.DataFrame(columns=['cluster', 'param', 'n_rules', 'precisionCF', 'recallCF', 'f1scoreCF', 'precisionComb', 'recallComb', 'f1scoreComb'])\n",
    "#train_data, test_data = load_files('user')\n",
    "train_data = np.load('./birch6_validate/user_train_data.npy').item()\n",
    "test_data = np.load('./birch6_final/user_test_data.npy').item()\n",
    "\n",
    "for cluster in range(0, 6):\n",
    "    print('cluster', cluster)\n",
    "    \n",
    "    if cluster == 2: continue\n",
    "    recommender = ItemCF(metric='cosine')\n",
    "    distance_matrix = recommender.fit(cluster, train_data[cluster])\n",
    "\n",
    "    freq_items, support, ass_rules = mine_rules(train_data[cluster], minSup=minSup_list[cluster], minConf=0.84, minKulc=0.88, maxIR=0.15)\n",
    "    \n",
    "    for param in param_list:\n",
    "        print('param', param)\n",
    "        new_ass_rules = ass_rules\n",
    "        n_rules = len(new_ass_rules)\n",
    "        precisionCF, recallCF, precisionComb, recallComb = recommendOnCombinationV1(2, param, recommender, new_ass_rules, support, \n",
    "                                                                            train_data[cluster], test_data[cluster], th=minParam)\n",
    "        results.loc[len(results)] = [cluster, param, n_rules, precisionCF, recallCF, 2*precisionCF*recallCF/(precisionCF+recallCF),precisionComb, recallComb, 2*precisionComb*recallComb/(precisionComb+recallComb)]\n",
    "\n",
    "results.to_csv(path+'birch_cf_arols_'+param_name+'_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
