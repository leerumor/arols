{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "# import modules & set up logging\n",
    "import gensim, logging\n",
    "\n",
    "def train_word_vecs(w2v_train, window_size, vector_size, save_path=''):\n",
    "    word2vec_model = gensim.models.Word2Vec(w2v_train, size=vector_size, window=window_size, min_count=5, workers=4, batch_words=128)\n",
    "    word_vectors = word2vec_model.wv\n",
    "    if save_path != '': word2vec_model.save(save_path)\n",
    "    del word2vec_model\n",
    "    return word_vectors\n",
    "\n",
    "def load_word_vecs(save_path):\n",
    "    word2vec_model = gensim.models.Word2Vec.load(save_path)\n",
    "    word_vectors = word2vec_model.wv\n",
    "    del word2vec_model\n",
    "    return word_vectors\n",
    "\n",
    "def load_sequence(train_data, window_size):\n",
    "    sequences = list()\n",
    "    length = window_size + 1\n",
    "    for user_id, items in train_data.items():\n",
    "        if len(items) < length: continue\n",
    "        for i in range(length, len(items)):\n",
    "            # select sequence of tokens\n",
    "            seq = items[i-length:i]\n",
    "            i+=2\n",
    "            # store\n",
    "            sequences.append(seq)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    return np.array(sequences)\n",
    "\n",
    "def item_to_vec(word_vectors, seqs, window_size):\n",
    "    result = list()\n",
    "    for seq in seqs:\n",
    "        tmp = list()\n",
    "        for item in seq:\n",
    "            if item not in word_vectors.vocab:\n",
    "                continue\n",
    "            tmp.append(word_vectors[item])\n",
    "        if len(tmp) != (window_size+1):\n",
    "            continue\n",
    "        result.append(np.array(tmp))\n",
    "    return np.asarray(result)\n",
    "\n",
    "def generate_sequences_3D(dict_data, window_size, word_vectors, drop_ratio=0):\n",
    "    # get sequences\n",
    "    sequences = load_sequence(dict_data, window_size)\n",
    "    if drop_ratio>0:\n",
    "        sequences, sequences_drop = train_test_split(sequences, test_size=drop_ratio, random_state=42)\n",
    "    sequences = item_to_vec(word_vectors, sequences, window_size)\n",
    "    X, y = sequences[:,:-1, :], sequences[:, -1, :]\n",
    "    return X, y\n",
    "\n",
    "def train_lstm_model_1(X_train, y_train, X_test, y_test, vector_size, n_units):\n",
    "    np.random.seed(42)\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_units, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dense(vector_size))\n",
    "    model.compile(loss='cosine_proximity', optimizer='adam')\n",
    "    # fit network\n",
    "    history = model.fit(X_train, y_train, epochs=20, batch_size=128, validation_data=(X_test, y_test), verbose=2, shuffle=False)\n",
    "    return history, model\n",
    "\n",
    "def train_lstm_model_2(X_train, y_train, X_test, y_test, vector_size, n_units):\n",
    "    np.random.seed(42)\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(Dense(vector_size))\n",
    "    model.compile(loss='cosine_proximity', optimizer='adam')\n",
    "    # fit network\n",
    "    history = model.fit(X_train, y_train, epochs=20, batch_size=128, validation_data=(X_test, y_test), verbose=2, shuffle=False)\n",
    "    return history, model\n",
    "\n",
    "def train_lstm_model_3(X_train, y_train, X_test, y_test, vector_size, n_units):\n",
    "    np.random.seed(42)\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(1024, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(1024, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(1024))\n",
    "    model.add(Dense(vector_size))\n",
    "    model.compile(loss='cosine_proximity', optimizer='adam')\n",
    "    # fit network\n",
    "    history = model.fit(X_train, y_train, epochs=20, batch_size=128, validation_data=(X_test, y_test), verbose=2, shuffle=False)\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepqre data\n",
    "path = './nlp_validate/birch6/'\n",
    "train_data = np.load(path + 'user_train_data.npy').item() #train_data[0] = dict(key=id_user, value=list of history)\n",
    "test_data = np.load(path + 'user_test_data.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.WARN)\n",
    "\n",
    "records = dict()\n",
    "window_size = 5\n",
    "vector_size = 128\n",
    "#records[vector_size] = dict()\n",
    "w2v_train = list()\n",
    "#for cluster in range(0, 6):\n",
    "#    w2v_train.extend(list(train_data[cluster].values()))\n",
    "#word_vectors = train_word_vecs(w2v_train, window_size, vector_size, save_path=path+'w2vmodel_'+str(vector_size))\n",
    "word_vectors = load_word_vecs(path+'w2vmodel_'+str(vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "units 128\n",
      "Total Sequences: 3211196\n",
      "Total Sequences: 313572\n",
      "Train on 64183 samples, validate on 6167 samples\n",
      "Epoch 1/20\n",
      " - 934s - loss: -6.8917e-01 - val_loss: -7.3393e-01\n",
      "Epoch 2/20\n",
      " - 864s - loss: -7.1203e-01 - val_loss: -7.3979e-01\n",
      "Epoch 3/20\n",
      " - 852s - loss: -7.1706e-01 - val_loss: -7.4150e-01\n",
      "Epoch 4/20\n",
      " - 928s - loss: -7.2088e-01 - val_loss: -7.4075e-01\n",
      "Epoch 5/20\n",
      " - 994s - loss: -7.2501e-01 - val_loss: -7.3942e-01\n",
      "Epoch 6/20\n",
      " - 914s - loss: -7.3024e-01 - val_loss: -7.3606e-01\n",
      "Epoch 7/20\n",
      " - 977s - loss: -7.3797e-01 - val_loss: -7.3293e-01\n",
      "Epoch 8/20\n",
      " - 1034s - loss: -7.4865e-01 - val_loss: -7.3004e-01\n",
      "Epoch 9/20\n",
      " - 922s - loss: -7.6299e-01 - val_loss: -7.2333e-01\n",
      "Epoch 10/20\n",
      " - 949s - loss: -7.7741e-01 - val_loss: -7.1788e-01\n",
      "Epoch 11/20\n",
      " - 931s - loss: -7.9419e-01 - val_loss: -7.0528e-01\n",
      "Epoch 12/20\n",
      " - 850s - loss: -8.1105e-01 - val_loss: -7.0058e-01\n",
      "Epoch 13/20\n",
      " - 853s - loss: -8.2702e-01 - val_loss: -6.9962e-01\n",
      "Epoch 14/20\n",
      " - 854s - loss: -8.4076e-01 - val_loss: -6.9707e-01\n",
      "Epoch 15/20\n",
      " - 860s - loss: -8.5317e-01 - val_loss: -6.9310e-01\n",
      "Epoch 16/20\n",
      " - 914s - loss: -8.6341e-01 - val_loss: -6.8695e-01\n",
      "Epoch 17/20\n",
      " - 916s - loss: -8.7190e-01 - val_loss: -6.8296e-01\n",
      "Epoch 18/20\n",
      " - 993s - loss: -8.7864e-01 - val_loss: -6.8455e-01\n",
      "Epoch 19/20\n",
      " - 934s - loss: -8.8293e-01 - val_loss: -6.7945e-01\n",
      "Epoch 20/20\n",
      " - 948s - loss: -8.8756e-01 - val_loss: -6.7945e-01\n",
      "0.18194444444444444 0.04409660860052175\n"
     ]
    }
   ],
   "source": [
    "#调参\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "def item_to_vec_2D(word_vectors, seq, window_size):\n",
    "    tmp = list()\n",
    "    for item in seq:\n",
    "        if item not in word_vectors.vocab:\n",
    "            return np.array([])\n",
    "        tmp.append(word_vectors[item])\n",
    "    return np.asarray(tmp)\n",
    "\n",
    "def evaluate(N, cluster, model):\n",
    "    window_size = 5\n",
    "    hit = 0\n",
    "    precision = 0.0\n",
    "    recall = 0.0\n",
    "    for user_id, itemset in test_data[cluster].items():\n",
    "        history = train_data[cluster][user_id]\n",
    "        if len(history)<window_size:continue\n",
    "        if np.random.random()>0.1: continue\n",
    "        history = item_to_vec_2D(word_vectors, history[-window_size:], window_size)\n",
    "        if len(history)==0: continue\n",
    "        history = history.reshape(1, window_size, vector_size)\n",
    "        precision += N\n",
    "        recall += len(itemset)\n",
    "        pred_vec = model.predict(history)\n",
    "        pred_set = word_vectors.most_similar(positive=pred_vec, topn=N)\n",
    "        for pred_item, score in pred_set:\n",
    "            if pred_item in itemset: \n",
    "                hit+=1\n",
    "    precision = hit/precision\n",
    "    recall = hit/recall\n",
    "    return precision, recall\n",
    "\n",
    "cluster = 0\n",
    "for n_units in [128]:\n",
    "    print('units', n_units)\n",
    "    X_train, y_train = generate_sequences_3D(train_data[cluster], window_size, word_vectors, drop_ratio=0.98)\n",
    "    X_test, y_test = generate_sequences_3D(test_data[cluster], window_size, word_vectors, drop_ratio=0.98)\n",
    "    history, model = train_lstm_model_3(X_train, y_train, X_test, y_test, vector_size, n_units)\n",
    "    precision, recall = evaluate(10, cluster, model)\n",
    "    del model\n",
    "    print(precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 3211196\n",
      "Total Sequences: 313572\n",
      "Train on 64183 samples, validate on 6167 samples\n",
      "Epoch 1/20\n",
      " - 11s - loss: -6.4999e-01 - val_loss: -7.1452e-01\n",
      "Epoch 2/20\n",
      " - 11s - loss: -6.9978e-01 - val_loss: -7.2861e-01\n",
      "Epoch 3/20\n",
      " - 10s - loss: -7.0679e-01 - val_loss: -7.3388e-01\n",
      "Epoch 4/20\n",
      " - 11s - loss: -7.1008e-01 - val_loss: -7.3657e-01\n",
      "Epoch 5/20\n",
      " - 10s - loss: -7.1209e-01 - val_loss: -7.3822e-01\n",
      "Epoch 6/20\n",
      " - 10s - loss: -7.1351e-01 - val_loss: -7.3937e-01\n",
      "Epoch 7/20\n",
      " - 11s - loss: -7.1460e-01 - val_loss: -7.4024e-01\n",
      "Epoch 8/20\n",
      " - 11s - loss: -7.1548e-01 - val_loss: -7.4092e-01\n",
      "Epoch 9/20\n",
      " - 11s - loss: -7.1621e-01 - val_loss: -7.4144e-01\n",
      "Epoch 10/20\n",
      " - 11s - loss: -7.1682e-01 - val_loss: -7.4186e-01\n",
      "Epoch 11/20\n",
      " - 10s - loss: -7.1733e-01 - val_loss: -7.4218e-01\n",
      "Epoch 12/20\n",
      " - 10s - loss: -7.1777e-01 - val_loss: -7.4244e-01\n",
      "Epoch 13/20\n",
      " - 10s - loss: -7.1816e-01 - val_loss: -7.4265e-01\n",
      "Epoch 14/20\n",
      " - 11s - loss: -7.1849e-01 - val_loss: -7.4283e-01\n",
      "Epoch 15/20\n",
      " - 10s - loss: -7.1880e-01 - val_loss: -7.4297e-01\n",
      "Epoch 16/20\n",
      " - 10s - loss: -7.1907e-01 - val_loss: -7.4309e-01\n",
      "Epoch 17/20\n",
      " - 11s - loss: -7.1932e-01 - val_loss: -7.4320e-01\n",
      "Epoch 18/20\n",
      " - 10s - loss: -7.1954e-01 - val_loss: -7.4326e-01\n",
      "Epoch 19/20\n",
      " - 10s - loss: -7.1976e-01 - val_loss: -7.4333e-01\n",
      "Epoch 20/20\n",
      " - 11s - loss: -7.1996e-01 - val_loss: -7.4338e-01\n"
     ]
    }
   ],
   "source": [
    "#训练模型\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "model_name = '64'\n",
    "\n",
    "for cluster in range(0, 1):\n",
    "    if cluster == 2: continue\n",
    "    X_train, y_train = generate_sequences_3D(train_data[cluster], window_size, word_vectors, drop_ratio=0.98)\n",
    "    X_test, y_test = generate_sequences_3D(test_data[cluster], window_size, word_vectors, drop_ratio=0.98)\n",
    "    history, model = train_lstm_model_1(X_train, y_train, X_test, y_test, vector_size)\n",
    "    model.save(path+'model0_'+model_name+'.h5')\n",
    "    del model\n",
    "    #records[cluster] = history\n",
    "\n",
    "#results = dict()\n",
    "#for k, v in records.items():\n",
    "#    results[k] = dict()\n",
    "#    results[k]['loss'] = v.history['loss']\n",
    "#    results[k]['val_loss'] = v.history['val_loss']\n",
    "#np.save(path + 'lstm_arols_final.npy', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1492896174863388 0.018875746186159628\n",
      "0.134037558685446 0.031660659828112\n",
      "0.12474389282899921 0.04378734233237442\n",
      "0.12275784753363228 0.05885672821091671\n",
      "0.11219730941704036 0.06882513135092014\n",
      "0.11441947565543072 0.08475281584641847\n",
      "0.10516643550624133 0.08399800598205384\n",
      "0.1072480181200453 0.09993931878743108\n",
      "0.10183852917665867 0.10873709067114284\n",
      "0.10410958904109589 0.12863187588152328\n",
      "0.09670096531240055 0.12947392341779343\n",
      "0.10116497557309283 0.1429329935223532\n"
     ]
    }
   ],
   "source": [
    "# 准确率，覆盖率测试\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "\n",
    "def item_to_vec_2D(word_vectors, seq, window_size):\n",
    "    tmp = list()\n",
    "    for item in seq:\n",
    "        if item not in word_vectors.vocab:\n",
    "            return np.array([])\n",
    "        tmp.append(word_vectors[item])\n",
    "    return np.asarray(tmp)\n",
    "\n",
    "cluster = 0\n",
    "window_size = 5\n",
    "\n",
    "model = model = load_model(path+'model0_'+model_name+'.h5')\n",
    "\n",
    "results = pd.DataFrame(columns=['N', 'precision', 'recall', 'f1score'])\n",
    "\n",
    "for N in range(5, 65, 5):\n",
    "    hit = 0\n",
    "    precision = 0.0\n",
    "    recall = 0.0\n",
    "    for user_id, itemset in test_data[cluster].items():\n",
    "        history = train_data[cluster][user_id]\n",
    "        if len(history)<window_size:continue\n",
    "        if np.random.random()>0.1: continue\n",
    "        history = item_to_vec_2D(word_vectors, history[-window_size:], window_size)\n",
    "        if len(history)==0: continue\n",
    "        history = history.reshape(1, window_size, vector_size)\n",
    "        precision += N\n",
    "        recall += len(itemset)\n",
    "        pred_vec = model.predict(history)\n",
    "        pred_set = word_vectors.most_similar(positive=pred_vec, topn=N)\n",
    "        for pred_item, score in pred_set:\n",
    "            if pred_item in itemset: \n",
    "                hit+=1\n",
    "    precision = hit/precision\n",
    "    recall = hit/recall\n",
    "    results.loc[len(results)] = [N, precision, recall, 2*recall*precision/(recall+precision)]\n",
    "    print(precision, recall)\n",
    "results.to_csv(path + 'lstm_arols_N_baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 3211196\n",
      "Total Sequences: 313572\n",
      "Train on 32091 samples, validate on 3083 samples\n",
      "Epoch 1/20\n",
      " - 12s - loss: -6.7101e-01 - val_loss: -7.3103e-01\n",
      "Epoch 2/20\n",
      " - 10s - loss: -7.0871e-01 - val_loss: -7.3802e-01\n",
      "Epoch 3/20\n",
      " - 10s - loss: -7.1383e-01 - val_loss: -7.4123e-01\n",
      "Epoch 4/20\n",
      " - 10s - loss: -7.1679e-01 - val_loss: -7.4294e-01\n",
      "Epoch 5/20\n",
      " - 10s - loss: -7.1897e-01 - val_loss: -7.4397e-01\n",
      "Epoch 6/20\n",
      " - 10s - loss: -7.2075e-01 - val_loss: -7.4455e-01\n",
      "Epoch 7/20\n",
      " - 10s - loss: -7.2232e-01 - val_loss: -7.4481e-01\n",
      "Epoch 8/20\n",
      " - 10s - loss: -7.2378e-01 - val_loss: -7.4481e-01\n",
      "Epoch 9/20\n",
      " - 13s - loss: -7.2519e-01 - val_loss: -7.4462e-01\n",
      "Epoch 10/20\n",
      " - 12s - loss: -7.2660e-01 - val_loss: -7.4432e-01\n",
      "Epoch 11/20\n",
      " - 11s - loss: -7.2803e-01 - val_loss: -7.4392e-01\n",
      "Epoch 12/20\n",
      " - 12s - loss: -7.2951e-01 - val_loss: -7.4341e-01\n",
      "Epoch 13/20\n",
      " - 11s - loss: -7.3107e-01 - val_loss: -7.4276e-01\n",
      "Epoch 14/20\n",
      " - 12s - loss: -7.3273e-01 - val_loss: -7.4191e-01\n",
      "Epoch 15/20\n",
      " - 12s - loss: -7.3450e-01 - val_loss: -7.4093e-01\n",
      "Epoch 16/20\n",
      " - 11s - loss: -7.3640e-01 - val_loss: -7.3974e-01\n",
      "Epoch 17/20\n",
      " - 11s - loss: -7.3846e-01 - val_loss: -7.3838e-01\n",
      "Epoch 18/20\n",
      " - 11s - loss: -7.4068e-01 - val_loss: -7.3689e-01\n",
      "Epoch 19/20\n",
      " - 11s - loss: -7.4307e-01 - val_loss: -7.3520e-01\n",
      "Epoch 20/20\n",
      " - 10s - loss: -7.4562e-01 - val_loss: -7.3331e-01\n",
      "Train on 32091 samples, validate on 3083 samples\n",
      "Epoch 1/20\n",
      " - 21s - loss: -6.6217e-01 - val_loss: -7.2601e-01\n",
      "Epoch 2/20\n",
      " - 18s - loss: -7.0283e-01 - val_loss: -7.3586e-01\n",
      "Epoch 3/20\n",
      " - 18s - loss: -7.0887e-01 - val_loss: -7.3960e-01\n",
      "Epoch 4/20\n",
      " - 18s - loss: -7.1223e-01 - val_loss: -7.4129e-01\n",
      "Epoch 5/20\n",
      " - 18s - loss: -7.1472e-01 - val_loss: -7.4268e-01\n",
      "Epoch 6/20\n",
      " - 18s - loss: -7.1679e-01 - val_loss: -7.4347e-01\n",
      "Epoch 7/20\n",
      " - 20s - loss: -7.1854e-01 - val_loss: -7.4359e-01\n",
      "Epoch 8/20\n",
      " - 18s - loss: -7.2024e-01 - val_loss: -7.4366e-01\n",
      "Epoch 9/20\n",
      " - 18s - loss: -7.2205e-01 - val_loss: -7.4371e-01\n",
      "Epoch 10/20\n",
      " - 18s - loss: -7.2380e-01 - val_loss: -7.4361e-01\n",
      "Epoch 11/20\n",
      " - 18s - loss: -7.2562e-01 - val_loss: -7.4309e-01\n",
      "Epoch 12/20\n",
      " - 19s - loss: -7.2778e-01 - val_loss: -7.4231e-01\n",
      "Epoch 13/20\n",
      " - 21s - loss: -7.3008e-01 - val_loss: -7.4113e-01\n",
      "Epoch 14/20\n",
      " - 21s - loss: -7.3263e-01 - val_loss: -7.4012e-01\n",
      "Epoch 15/20\n",
      " - 19s - loss: -7.3552e-01 - val_loss: -7.3883e-01\n",
      "Epoch 16/20\n",
      " - 20s - loss: -7.3870e-01 - val_loss: -7.3704e-01\n",
      "Epoch 17/20\n",
      " - 20s - loss: -7.4254e-01 - val_loss: -7.3506e-01\n",
      "Epoch 18/20\n",
      " - 22s - loss: -7.4632e-01 - val_loss: -7.3358e-01\n",
      "Epoch 19/20\n",
      " - 21s - loss: -7.5078e-01 - val_loss: -7.3021e-01\n",
      "Epoch 20/20\n",
      " - 20s - loss: -7.5543e-01 - val_loss: -7.2914e-01\n",
      "Train on 32091 samples, validate on 3083 samples\n",
      "Epoch 1/20\n",
      " - 36s - loss: -6.6404e-01 - val_loss: -7.2433e-01\n",
      "Epoch 2/20\n",
      " - 30s - loss: -6.9918e-01 - val_loss: -7.3306e-01\n",
      "Epoch 3/20\n",
      " - 31s - loss: -7.0523e-01 - val_loss: -7.3687e-01\n",
      "Epoch 4/20\n",
      " - 30s - loss: -7.0916e-01 - val_loss: -7.3932e-01\n",
      "Epoch 5/20\n",
      " - 31s - loss: -7.1210e-01 - val_loss: -7.4058e-01\n",
      "Epoch 6/20\n",
      " - 29s - loss: -7.1455e-01 - val_loss: -7.4174e-01\n",
      "Epoch 7/20\n",
      " - 28s - loss: -7.1674e-01 - val_loss: -7.4194e-01\n",
      "Epoch 8/20\n",
      " - 33s - loss: -7.1889e-01 - val_loss: -7.4205e-01\n",
      "Epoch 9/20\n",
      " - 30s - loss: -7.2122e-01 - val_loss: -7.4177e-01\n",
      "Epoch 10/20\n",
      " - 31s - loss: -7.2358e-01 - val_loss: -7.4176e-01\n",
      "Epoch 11/20\n",
      " - 30s - loss: -7.2651e-01 - val_loss: -7.4105e-01\n",
      "Epoch 12/20\n",
      " - 31s - loss: -7.2937e-01 - val_loss: -7.4095e-01\n",
      "Epoch 13/20\n",
      " - 33s - loss: -7.3313e-01 - val_loss: -7.3951e-01\n",
      "Epoch 14/20\n",
      " - 32s - loss: -7.3725e-01 - val_loss: -7.3640e-01\n",
      "Epoch 15/20\n",
      " - 32s - loss: -7.4163e-01 - val_loss: -7.3512e-01\n",
      "Epoch 16/20\n",
      " - 30s - loss: -7.4691e-01 - val_loss: -7.3156e-01\n",
      "Epoch 17/20\n",
      " - 35s - loss: -7.5208e-01 - val_loss: -7.3033e-01\n",
      "Epoch 18/20\n",
      " - 31s - loss: -7.5792e-01 - val_loss: -7.2814e-01\n",
      "Epoch 19/20\n",
      " - 30s - loss: -7.6274e-01 - val_loss: -7.2537e-01\n",
      "Epoch 20/20\n",
      " - 32s - loss: -7.6817e-01 - val_loss: -7.2403e-01\n"
     ]
    }
   ],
   "source": [
    "#测试lstm层数\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "for cluster in range(0, 1):\n",
    "    if cluster == 2: continue\n",
    "    records[cluster] = dict()\n",
    "    X_train, y_train = generate_sequences_3D(train_data[cluster], window_size, word_vectors, drop_ratio=0.99)\n",
    "    X_test, y_test = generate_sequences_3D(test_data[cluster], window_size, word_vectors, drop_ratio=0.99)\n",
    "    history, model = train_lstm_model_1(X_train, y_train, X_test, y_test, vector_size)\n",
    "    del model\n",
    "    records[cluster][1] = history\n",
    "    history, model = train_lstm_model_2(X_train, y_train, X_test, y_test, vector_size)\n",
    "    del model\n",
    "    records[cluster][2] = history\n",
    "    history, model = train_lstm_model_3(X_train, y_train, X_test, y_test, vector_size)\n",
    "    del model\n",
    "    records[cluster][3] = history\n",
    "\n",
    "results = dict()\n",
    "for cluster in range(0, 1):\n",
    "    if cluster == 2: continue\n",
    "    results[cluster] = dict()\n",
    "    for k, v in records[cluster].items():\n",
    "        results[cluster][k] = dict()\n",
    "        results[cluster][k]['loss'] = v.history['loss']\n",
    "        results[cluster][k]['val_loss'] = v.history['val_loss']\n",
    "np.save(path + 'lstm_word2vec_n_layers_256_256128_256128256.npy', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 3211196\n",
      "Total Sequences: 313572\n",
      "Train on 32091 samples, validate on 3083 samples\n",
      "Epoch 1/20\n",
      " - 164s - loss: -6.8552e-01 - val_loss: -7.2831e-01\n",
      "Epoch 2/20\n",
      " - 162s - loss: -7.0518e-01 - val_loss: -7.3323e-01\n",
      "Epoch 3/20\n",
      " - 162s - loss: -7.0898e-01 - val_loss: -7.3500e-01\n",
      "Epoch 4/20\n",
      " - 164s - loss: -7.1119e-01 - val_loss: -7.3567e-01\n",
      "Epoch 5/20\n",
      " - 164s - loss: -7.1278e-01 - val_loss: -7.3592e-01\n",
      "Epoch 6/20\n",
      " - 164s - loss: -7.1402e-01 - val_loss: -7.3601e-01\n",
      "Epoch 7/20\n",
      " - 166s - loss: -7.1503e-01 - val_loss: -7.3583e-01\n",
      "Epoch 8/20\n",
      " - 167s - loss: -7.1590e-01 - val_loss: -7.3555e-01\n",
      "Epoch 9/20\n",
      " - 167s - loss: -7.1667e-01 - val_loss: -7.3538e-01\n",
      "Epoch 10/20\n",
      " - 164s - loss: -7.1738e-01 - val_loss: -7.3493e-01\n",
      "Epoch 11/20\n",
      " - 163s - loss: -7.1806e-01 - val_loss: -7.3447e-01\n",
      "Epoch 12/20\n",
      " - 163s - loss: -7.1869e-01 - val_loss: -7.3398e-01\n",
      "Epoch 13/20\n",
      " - 165s - loss: -7.1928e-01 - val_loss: -7.3350e-01\n",
      "Epoch 14/20\n",
      " - 166s - loss: -7.1984e-01 - val_loss: -7.3320e-01\n",
      "Epoch 15/20\n",
      " - 166s - loss: -7.2037e-01 - val_loss: -7.3273e-01\n",
      "Epoch 16/20\n",
      " - 168s - loss: -7.2090e-01 - val_loss: -7.3214e-01\n",
      "Epoch 17/20\n",
      " - 167s - loss: -7.2141e-01 - val_loss: -7.3190e-01\n",
      "Epoch 18/20\n",
      " - 162s - loss: -7.2190e-01 - val_loss: -7.3129e-01\n",
      "Epoch 19/20\n",
      " - 163s - loss: -7.2238e-01 - val_loss: -7.3062e-01\n",
      "Epoch 20/20\n",
      " - 162s - loss: -7.2279e-01 - val_loss: -7.3021e-01\n",
      "Train on 32091 samples, validate on 3083 samples\n",
      "Epoch 1/20\n",
      " - 167s - loss: -6.9350e-01 - val_loss: -7.3188e-01\n",
      "Epoch 2/20\n",
      " - 173s - loss: -7.1008e-01 - val_loss: -7.3601e-01\n",
      "Epoch 3/20\n",
      " - 171s - loss: -7.1485e-01 - val_loss: -7.3715e-01\n",
      "Epoch 4/20\n",
      " - 170s - loss: -7.1814e-01 - val_loss: -7.3729e-01\n",
      "Epoch 5/20\n",
      " - 174s - loss: -7.2078e-01 - val_loss: -7.3694e-01\n",
      "Epoch 6/20\n",
      " - 175s - loss: -7.2319e-01 - val_loss: -7.3626e-01\n",
      "Epoch 7/20\n",
      " - 176s - loss: -7.2544e-01 - val_loss: -7.3537e-01\n",
      "Epoch 8/20\n",
      " - 180s - loss: -7.2762e-01 - val_loss: -7.3422e-01\n",
      "Epoch 9/20\n",
      " - 170s - loss: -7.2965e-01 - val_loss: -7.3292e-01\n",
      "Epoch 10/20\n",
      " - 172s - loss: -7.3170e-01 - val_loss: -7.3147e-01\n",
      "Epoch 11/20\n",
      " - 170s - loss: -7.3362e-01 - val_loss: -7.3011e-01\n",
      "Epoch 12/20\n",
      " - 172s - loss: -7.3541e-01 - val_loss: -7.2929e-01\n",
      "Epoch 13/20\n",
      " - 172s - loss: -7.3726e-01 - val_loss: -7.2801e-01\n",
      "Epoch 14/20\n",
      " - 175s - loss: -7.3903e-01 - val_loss: -7.2666e-01\n",
      "Epoch 15/20\n",
      " - 174s - loss: -7.4069e-01 - val_loss: -7.2543e-01\n",
      "Epoch 16/20\n",
      " - 172s - loss: -7.4221e-01 - val_loss: -7.2429e-01\n",
      "Epoch 17/20\n",
      " - 167s - loss: -7.4367e-01 - val_loss: -7.2363e-01\n",
      "Epoch 18/20\n",
      " - 166s - loss: -7.4511e-01 - val_loss: -7.2211e-01\n",
      "Epoch 19/20\n",
      " - 167s - loss: -7.4630e-01 - val_loss: -7.2093e-01\n",
      "Epoch 20/20\n",
      " - 167s - loss: -7.4744e-01 - val_loss: -7.2027e-01\n",
      "Train on 32091 samples, validate on 3083 samples\n",
      "Epoch 1/20\n",
      " - 205s - loss: -6.9705e-01 - val_loss: -7.3505e-01\n",
      "Epoch 2/20\n",
      " - 204s - loss: -7.1264e-01 - val_loss: -7.3870e-01\n",
      "Epoch 3/20\n",
      " - 204s - loss: -7.1889e-01 - val_loss: -7.3865e-01\n",
      "Epoch 4/20\n",
      " - 204s - loss: -7.2440e-01 - val_loss: -7.3739e-01\n",
      "Epoch 5/20\n",
      " - 204s - loss: -7.2982e-01 - val_loss: -7.3537e-01\n",
      "Epoch 6/20\n",
      " - 204s - loss: -7.3548e-01 - val_loss: -7.3189e-01\n",
      "Epoch 7/20\n",
      " - 204s - loss: -7.4112e-01 - val_loss: -7.2886e-01\n",
      "Epoch 8/20\n",
      " - 204s - loss: -7.4685e-01 - val_loss: -7.2491e-01\n",
      "Epoch 9/20\n",
      " - 204s - loss: -7.5242e-01 - val_loss: -7.2192e-01\n",
      "Epoch 10/20\n",
      " - 204s - loss: -7.5779e-01 - val_loss: -7.1979e-01\n",
      "Epoch 11/20\n",
      " - 204s - loss: -7.6272e-01 - val_loss: -7.1667e-01\n",
      "Epoch 12/20\n",
      " - 208s - loss: -7.6704e-01 - val_loss: -7.1495e-01\n",
      "Epoch 13/20\n",
      " - 213s - loss: -7.7085e-01 - val_loss: -7.1276e-01\n",
      "Epoch 14/20\n",
      " - 204s - loss: -7.7408e-01 - val_loss: -7.1038e-01\n",
      "Epoch 15/20\n",
      " - 204s - loss: -7.7698e-01 - val_loss: -7.0978e-01\n",
      "Epoch 16/20\n",
      " - 220s - loss: -7.7962e-01 - val_loss: -7.0805e-01\n",
      "Epoch 17/20\n",
      " - 214s - loss: -7.8197e-01 - val_loss: -7.0757e-01\n",
      "Epoch 18/20\n",
      " - 211s - loss: -7.8383e-01 - val_loss: -7.0505e-01\n",
      "Epoch 19/20\n",
      " - 232s - loss: -7.8594e-01 - val_loss: -7.0394e-01\n",
      "Epoch 20/20\n",
      " - 227s - loss: -7.8736e-01 - val_loss: -7.0270e-01\n",
      "Total Sequences: 1350191\n",
      "Total Sequences: 121710\n",
      "Train on 13488 samples, validate on 1211 samples\n",
      "Epoch 1/20\n",
      " - 70s - loss: -7.0915e-01 - val_loss: -7.3423e-01\n",
      "Epoch 2/20\n",
      " - 70s - loss: -7.3555e-01 - val_loss: -7.4144e-01\n",
      "Epoch 3/20\n",
      " - 68s - loss: -7.4113e-01 - val_loss: -7.4456e-01\n",
      "Epoch 4/20\n",
      " - 69s - loss: -7.4465e-01 - val_loss: -7.4581e-01\n",
      "Epoch 5/20\n",
      " - 69s - loss: -7.4720e-01 - val_loss: -7.4612e-01\n",
      "Epoch 6/20\n",
      " - 66s - loss: -7.4921e-01 - val_loss: -7.4597e-01\n",
      "Epoch 7/20\n",
      " - 69s - loss: -7.5089e-01 - val_loss: -7.4557e-01\n",
      "Epoch 8/20\n",
      " - 68s - loss: -7.5238e-01 - val_loss: -7.4504e-01\n",
      "Epoch 9/20\n",
      " - 67s - loss: -7.5372e-01 - val_loss: -7.4451e-01\n",
      "Epoch 10/20\n",
      " - 70s - loss: -7.5496e-01 - val_loss: -7.4404e-01\n",
      "Epoch 11/20\n",
      " - 70s - loss: -7.5610e-01 - val_loss: -7.4352e-01\n",
      "Epoch 12/20\n",
      " - 68s - loss: -7.5719e-01 - val_loss: -7.4306e-01\n",
      "Epoch 13/20\n",
      " - 69s - loss: -7.5822e-01 - val_loss: -7.4253e-01\n",
      "Epoch 14/20\n",
      " - 68s - loss: -7.5922e-01 - val_loss: -7.4208e-01\n",
      "Epoch 15/20\n",
      " - 69s - loss: -7.6020e-01 - val_loss: -7.4153e-01\n",
      "Epoch 16/20\n",
      " - 70s - loss: -7.6113e-01 - val_loss: -7.4066e-01\n",
      "Epoch 17/20\n",
      " - 68s - loss: -7.6202e-01 - val_loss: -7.3999e-01\n",
      "Epoch 18/20\n",
      " - 69s - loss: -7.6291e-01 - val_loss: -7.3933e-01\n",
      "Epoch 19/20\n",
      " - 70s - loss: -7.6376e-01 - val_loss: -7.3867e-01\n",
      "Epoch 20/20\n",
      " - 69s - loss: -7.6457e-01 - val_loss: -7.3814e-01\n",
      "Train on 13488 samples, validate on 1211 samples\n",
      "Epoch 1/20\n",
      " - 75s - loss: -7.1917e-01 - val_loss: -7.4110e-01\n",
      "Epoch 2/20\n",
      " - 72s - loss: -7.4050e-01 - val_loss: -7.4648e-01\n",
      "Epoch 3/20\n",
      " - 70s - loss: -7.4678e-01 - val_loss: -7.4842e-01\n",
      "Epoch 4/20\n",
      " - 72s - loss: -7.5139e-01 - val_loss: -7.4859e-01\n",
      "Epoch 5/20\n",
      " - 72s - loss: -7.5532e-01 - val_loss: -7.4810e-01\n",
      "Epoch 6/20\n",
      " - 72s - loss: -7.5898e-01 - val_loss: -7.4712e-01\n",
      "Epoch 7/20\n",
      " - 71s - loss: -7.6242e-01 - val_loss: -7.4536e-01\n",
      "Epoch 8/20\n",
      " - 68s - loss: -7.6576e-01 - val_loss: -7.4364e-01\n",
      "Epoch 9/20\n",
      " - 71s - loss: -7.6899e-01 - val_loss: -7.4171e-01\n",
      "Epoch 10/20\n",
      " - 71s - loss: -7.7216e-01 - val_loss: -7.3966e-01\n",
      "Epoch 11/20\n",
      " - 72s - loss: -7.7523e-01 - val_loss: -7.3744e-01\n",
      "Epoch 12/20\n",
      " - 73s - loss: -7.7830e-01 - val_loss: -7.3518e-01\n",
      "Epoch 13/20\n",
      " - 71s - loss: -7.8127e-01 - val_loss: -7.3329e-01\n",
      "Epoch 14/20\n",
      " - 71s - loss: -7.8410e-01 - val_loss: -7.3119e-01\n",
      "Epoch 15/20\n",
      " - 72s - loss: -7.8682e-01 - val_loss: -7.2942e-01\n",
      "Epoch 16/20\n",
      " - 71s - loss: -7.8950e-01 - val_loss: -7.2667e-01\n",
      "Epoch 17/20\n",
      " - 71s - loss: -7.9206e-01 - val_loss: -7.2415e-01\n",
      "Epoch 18/20\n",
      " - 71s - loss: -7.9441e-01 - val_loss: -7.2204e-01\n",
      "Epoch 19/20\n",
      " - 70s - loss: -7.9675e-01 - val_loss: -7.1978e-01\n",
      "Epoch 20/20\n",
      " - 71s - loss: -7.9888e-01 - val_loss: -7.1829e-01\n",
      "Train on 13488 samples, validate on 1211 samples\n",
      "Epoch 1/20\n",
      " - 88s - loss: -7.2378e-01 - val_loss: -7.4325e-01\n",
      "Epoch 2/20\n",
      " - 86s - loss: -7.4279e-01 - val_loss: -7.4744e-01\n",
      "Epoch 3/20\n",
      " - 86s - loss: -7.5067e-01 - val_loss: -7.4837e-01\n",
      "Epoch 4/20\n",
      " - 93s - loss: -7.5770e-01 - val_loss: -7.4733e-01\n",
      "Epoch 5/20\n",
      " - 96s - loss: -7.6485e-01 - val_loss: -7.4503e-01\n",
      "Epoch 6/20\n",
      " - 90s - loss: -7.7247e-01 - val_loss: -7.4144e-01\n",
      "Epoch 7/20\n",
      " - 90s - loss: -7.8039e-01 - val_loss: -7.3777e-01\n",
      "Epoch 8/20\n",
      " - 90s - loss: -7.8848e-01 - val_loss: -7.3267e-01\n",
      "Epoch 9/20\n",
      " - 90s - loss: -7.9660e-01 - val_loss: -7.2773e-01\n",
      "Epoch 10/20\n",
      " - 92s - loss: -8.0443e-01 - val_loss: -7.2181e-01\n",
      "Epoch 11/20\n",
      " - 91s - loss: -8.1190e-01 - val_loss: -7.1719e-01\n",
      "Epoch 12/20\n",
      " - 85s - loss: -8.1858e-01 - val_loss: -7.1265e-01\n",
      "Epoch 13/20\n",
      " - 94s - loss: -8.2482e-01 - val_loss: -7.0926e-01\n",
      "Epoch 14/20\n",
      " - 98s - loss: -8.3021e-01 - val_loss: -7.0531e-01\n",
      "Epoch 15/20\n",
      " - 97s - loss: -8.3519e-01 - val_loss: -6.9990e-01\n",
      "Epoch 16/20\n",
      " - 97s - loss: -8.3953e-01 - val_loss: -6.9802e-01\n",
      "Epoch 17/20\n",
      " - 94s - loss: -8.4360e-01 - val_loss: -6.9820e-01\n",
      "Epoch 18/20\n",
      " - 97s - loss: -8.4725e-01 - val_loss: -6.9346e-01\n",
      "Epoch 19/20\n",
      " - 96s - loss: -8.5055e-01 - val_loss: -6.9157e-01\n",
      "Epoch 20/20\n",
      " - 96s - loss: -8.5354e-01 - val_loss: -6.8710e-01\n",
      "Total Sequences: 1259660\n",
      "Total Sequences: 117122\n",
      "Train on 12586 samples, validate on 1162 samples\n",
      "Epoch 1/20\n",
      " - 65s - loss: -7.5262e-01 - val_loss: -7.8778e-01\n",
      "Epoch 2/20\n",
      " - 64s - loss: -7.8064e-01 - val_loss: -7.9284e-01\n",
      "Epoch 3/20\n",
      " - 64s - loss: -7.8588e-01 - val_loss: -7.9441e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20\n",
      " - 64s - loss: -7.8899e-01 - val_loss: -7.9494e-01\n",
      "Epoch 5/20\n",
      " - 63s - loss: -7.9122e-01 - val_loss: -7.9500e-01\n",
      "Epoch 6/20\n",
      " - 63s - loss: -7.9299e-01 - val_loss: -7.9498e-01\n",
      "Epoch 7/20\n",
      " - 65s - loss: -7.9447e-01 - val_loss: -7.9484e-01\n",
      "Epoch 8/20\n",
      " - 64s - loss: -7.9579e-01 - val_loss: -7.9437e-01\n",
      "Epoch 9/20\n",
      " - 63s - loss: -7.9697e-01 - val_loss: -7.9407e-01\n",
      "Epoch 10/20\n",
      " - 64s - loss: -7.9806e-01 - val_loss: -7.9352e-01\n",
      "Epoch 11/20\n",
      " - 64s - loss: -7.9908e-01 - val_loss: -7.9296e-01\n",
      "Epoch 12/20\n",
      " - 60s - loss: -8.0006e-01 - val_loss: -7.9238e-01\n",
      "Epoch 13/20\n",
      " - 63s - loss: -8.0099e-01 - val_loss: -7.9175e-01\n",
      "Epoch 14/20\n",
      " - 65s - loss: -8.0186e-01 - val_loss: -7.9108e-01\n",
      "Epoch 15/20\n",
      " - 66s - loss: -8.0270e-01 - val_loss: -7.9044e-01\n",
      "Epoch 16/20\n",
      " - 64s - loss: -8.0350e-01 - val_loss: -7.8985e-01\n",
      "Epoch 17/20\n",
      " - 64s - loss: -8.0427e-01 - val_loss: -7.8920e-01\n",
      "Epoch 18/20\n",
      " - 64s - loss: -8.0502e-01 - val_loss: -7.8879e-01\n",
      "Epoch 19/20\n",
      " - 63s - loss: -8.0576e-01 - val_loss: -7.8811e-01\n",
      "Epoch 20/20\n",
      " - 63s - loss: -8.0649e-01 - val_loss: -7.8760e-01\n",
      "Train on 12586 samples, validate on 1162 samples\n",
      "Epoch 1/20\n",
      " - 68s - loss: -7.6312e-01 - val_loss: -7.9278e-01\n",
      "Epoch 2/20\n",
      " - 66s - loss: -7.8487e-01 - val_loss: -7.9619e-01\n",
      "Epoch 3/20\n",
      " - 66s - loss: -7.9034e-01 - val_loss: -7.9683e-01\n",
      "Epoch 4/20\n",
      " - 66s - loss: -7.9434e-01 - val_loss: -7.9638e-01\n",
      "Epoch 5/20\n",
      " - 66s - loss: -7.9773e-01 - val_loss: -7.9553e-01\n",
      "Epoch 6/20\n",
      " - 66s - loss: -8.0077e-01 - val_loss: -7.9457e-01\n",
      "Epoch 7/20\n",
      " - 66s - loss: -8.0358e-01 - val_loss: -7.9331e-01\n",
      "Epoch 8/20\n",
      " - 66s - loss: -8.0630e-01 - val_loss: -7.9200e-01\n",
      "Epoch 9/20\n",
      " - 66s - loss: -8.0892e-01 - val_loss: -7.9025e-01\n",
      "Epoch 10/20\n",
      " - 66s - loss: -8.1149e-01 - val_loss: -7.8890e-01\n",
      "Epoch 11/20\n",
      " - 66s - loss: -8.1405e-01 - val_loss: -7.8605e-01\n",
      "Epoch 12/20\n",
      " - 66s - loss: -8.1658e-01 - val_loss: -7.8448e-01\n",
      "Epoch 13/20\n",
      " - 66s - loss: -8.1904e-01 - val_loss: -7.8275e-01\n",
      "Epoch 14/20\n",
      " - 66s - loss: -8.2144e-01 - val_loss: -7.8129e-01\n",
      "Epoch 15/20\n",
      " - 66s - loss: -8.2385e-01 - val_loss: -7.7922e-01\n",
      "Epoch 16/20\n",
      " - 66s - loss: -8.2607e-01 - val_loss: -7.7899e-01\n",
      "Epoch 17/20\n",
      " - 66s - loss: -8.2815e-01 - val_loss: -7.7623e-01\n",
      "Epoch 18/20\n",
      " - 66s - loss: -8.3028e-01 - val_loss: -7.7532e-01\n",
      "Epoch 19/20\n",
      " - 66s - loss: -8.3243e-01 - val_loss: -7.7380e-01\n",
      "Epoch 20/20\n",
      " - 66s - loss: -8.3432e-01 - val_loss: -7.7215e-01\n",
      "Train on 12586 samples, validate on 1162 samples\n",
      "Epoch 1/20\n",
      " - 82s - loss: -7.6809e-01 - val_loss: -7.9367e-01\n",
      "Epoch 2/20\n",
      " - 80s - loss: -7.8711e-01 - val_loss: -7.9515e-01\n",
      "Epoch 3/20\n",
      " - 80s - loss: -7.9378e-01 - val_loss: -7.9518e-01\n",
      "Epoch 4/20\n",
      " - 80s - loss: -7.9959e-01 - val_loss: -7.9371e-01\n",
      "Epoch 5/20\n",
      " - 80s - loss: -8.0533e-01 - val_loss: -7.9015e-01\n",
      "Epoch 6/20\n",
      " - 80s - loss: -8.1130e-01 - val_loss: -7.8663e-01\n",
      "Epoch 7/20\n",
      " - 81s - loss: -8.1745e-01 - val_loss: -7.8416e-01\n",
      "Epoch 8/20\n",
      " - 80s - loss: -8.2349e-01 - val_loss: -7.7998e-01\n",
      "Epoch 9/20\n",
      " - 80s - loss: -8.2958e-01 - val_loss: -7.7528e-01\n",
      "Epoch 10/20\n",
      " - 80s - loss: -8.3585e-01 - val_loss: -7.7154e-01\n",
      "Epoch 11/20\n",
      " - 80s - loss: -8.4166e-01 - val_loss: -7.6765e-01\n",
      "Epoch 12/20\n",
      " - 80s - loss: -8.4729e-01 - val_loss: -7.6692e-01\n",
      "Epoch 13/20\n",
      " - 81s - loss: -8.5238e-01 - val_loss: -7.6268e-01\n",
      "Epoch 14/20\n",
      " - 95s - loss: -8.5699e-01 - val_loss: -7.5969e-01\n",
      "Epoch 15/20\n",
      " - 109s - loss: -8.6149e-01 - val_loss: -7.5923e-01\n",
      "Epoch 16/20\n",
      " - 98s - loss: -8.6517e-01 - val_loss: -7.5534e-01\n",
      "Epoch 17/20\n",
      " - 86s - loss: -8.6866e-01 - val_loss: -7.5428e-01\n",
      "Epoch 18/20\n",
      " - 85s - loss: -8.7220e-01 - val_loss: -7.5041e-01\n",
      "Epoch 19/20\n",
      " - 91s - loss: -8.7527e-01 - val_loss: -7.5152e-01\n",
      "Epoch 20/20\n",
      " - 93s - loss: -8.7796e-01 - val_loss: -7.5088e-01\n",
      "Total Sequences: 792361\n",
      "Total Sequences: 76744\n",
      "Train on 7911 samples, validate on 766 samples\n",
      "Epoch 1/20\n",
      " - 45s - loss: -7.5598e-01 - val_loss: -7.9509e-01\n",
      "Epoch 2/20\n",
      " - 41s - loss: -7.8667e-01 - val_loss: -8.0402e-01\n",
      "Epoch 3/20\n",
      " - 41s - loss: -7.9256e-01 - val_loss: -8.0726e-01\n",
      "Epoch 4/20\n",
      " - 41s - loss: -7.9617e-01 - val_loss: -8.0865e-01\n",
      "Epoch 5/20\n",
      " - 41s - loss: -7.9898e-01 - val_loss: -8.0918e-01\n",
      "Epoch 6/20\n",
      " - 41s - loss: -8.0127e-01 - val_loss: -8.0933e-01\n",
      "Epoch 7/20\n",
      " - 41s - loss: -8.0325e-01 - val_loss: -8.0931e-01\n",
      "Epoch 8/20\n",
      " - 40s - loss: -8.0499e-01 - val_loss: -8.0868e-01\n",
      "Epoch 9/20\n",
      " - 41s - loss: -8.0657e-01 - val_loss: -8.0799e-01\n",
      "Epoch 10/20\n",
      " - 41s - loss: -8.0800e-01 - val_loss: -8.0746e-01\n",
      "Epoch 11/20\n",
      " - 40s - loss: -8.0934e-01 - val_loss: -8.0660e-01\n",
      "Epoch 12/20\n",
      " - 40s - loss: -8.1055e-01 - val_loss: -8.0600e-01\n",
      "Epoch 13/20\n",
      " - 40s - loss: -8.1176e-01 - val_loss: -8.0508e-01\n",
      "Epoch 14/20\n",
      " - 40s - loss: -8.1287e-01 - val_loss: -8.0364e-01\n",
      "Epoch 15/20\n",
      " - 40s - loss: -8.1399e-01 - val_loss: -8.0295e-01\n",
      "Epoch 16/20\n",
      " - 40s - loss: -8.1501e-01 - val_loss: -8.0228e-01\n",
      "Epoch 17/20\n",
      " - 40s - loss: -8.1603e-01 - val_loss: -8.0096e-01\n",
      "Epoch 18/20\n",
      " - 41s - loss: -8.1699e-01 - val_loss: -8.0089e-01\n",
      "Epoch 19/20\n",
      " - 40s - loss: -8.1792e-01 - val_loss: -7.9964e-01\n",
      "Epoch 20/20\n",
      " - 40s - loss: -8.1882e-01 - val_loss: -7.9875e-01\n",
      "Train on 7911 samples, validate on 766 samples\n",
      "Epoch 1/20\n",
      " - 46s - loss: -7.6832e-01 - val_loss: -8.0146e-01\n",
      "Epoch 2/20\n",
      " - 42s - loss: -7.9118e-01 - val_loss: -8.0809e-01\n",
      "Epoch 3/20\n",
      " - 42s - loss: -7.9693e-01 - val_loss: -8.1035e-01\n",
      "Epoch 4/20\n",
      " - 43s - loss: -8.0121e-01 - val_loss: -8.1140e-01\n",
      "Epoch 5/20\n",
      " - 46s - loss: -8.0490e-01 - val_loss: -8.1162e-01\n",
      "Epoch 6/20\n",
      " - 42s - loss: -8.0834e-01 - val_loss: -8.1137e-01\n",
      "Epoch 7/20\n",
      " - 40s - loss: -8.1161e-01 - val_loss: -8.1047e-01\n",
      "Epoch 8/20\n",
      " - 41s - loss: -8.1479e-01 - val_loss: -8.0921e-01\n",
      "Epoch 9/20\n",
      " - 41s - loss: -8.1790e-01 - val_loss: -8.0836e-01\n",
      "Epoch 10/20\n",
      " - 40s - loss: -8.2083e-01 - val_loss: -8.0711e-01\n",
      "Epoch 11/20\n",
      " - 41s - loss: -8.2370e-01 - val_loss: -8.0592e-01\n",
      "Epoch 12/20\n",
      " - 40s - loss: -8.2642e-01 - val_loss: -8.0440e-01\n",
      "Epoch 13/20\n",
      " - 40s - loss: -8.2908e-01 - val_loss: -8.0282e-01\n",
      "Epoch 14/20\n",
      " - 40s - loss: -8.3156e-01 - val_loss: -8.0201e-01\n",
      "Epoch 15/20\n",
      " - 41s - loss: -8.3386e-01 - val_loss: -8.0009e-01\n",
      "Epoch 16/20\n",
      " - 40s - loss: -8.3616e-01 - val_loss: -7.9938e-01\n",
      "Epoch 17/20\n",
      " - 40s - loss: -8.3842e-01 - val_loss: -7.9781e-01\n",
      "Epoch 18/20\n",
      " - 40s - loss: -8.4061e-01 - val_loss: -7.9609e-01\n",
      "Epoch 19/20\n",
      " - 41s - loss: -8.4279e-01 - val_loss: -7.9405e-01\n",
      "Epoch 20/20\n",
      " - 43s - loss: -8.4480e-01 - val_loss: -7.9198e-01\n",
      "Train on 7911 samples, validate on 766 samples\n",
      "Epoch 1/20\n",
      " - 58s - loss: -7.7379e-01 - val_loss: -8.0391e-01\n",
      "Epoch 2/20\n",
      " - 53s - loss: -7.9290e-01 - val_loss: -8.0910e-01\n",
      "Epoch 3/20\n",
      " - 54s - loss: -7.9953e-01 - val_loss: -8.0969e-01\n",
      "Epoch 4/20\n",
      " - 53s - loss: -8.0547e-01 - val_loss: -8.0882e-01\n",
      "Epoch 5/20\n",
      " - 51s - loss: -8.1122e-01 - val_loss: -8.0831e-01\n",
      "Epoch 6/20\n",
      " - 51s - loss: -8.1710e-01 - val_loss: -8.0541e-01\n",
      "Epoch 7/20\n",
      " - 51s - loss: -8.2299e-01 - val_loss: -8.0393e-01\n",
      "Epoch 8/20\n",
      " - 51s - loss: -8.2884e-01 - val_loss: -8.0072e-01\n",
      "Epoch 9/20\n",
      " - 58s - loss: -8.3466e-01 - val_loss: -7.9840e-01\n",
      "Epoch 10/20\n",
      " - 56s - loss: -8.4033e-01 - val_loss: -7.9481e-01\n",
      "Epoch 11/20\n",
      " - 54s - loss: -8.4588e-01 - val_loss: -7.9247e-01\n",
      "Epoch 12/20\n",
      " - 55s - loss: -8.5152e-01 - val_loss: -7.8843e-01\n",
      "Epoch 13/20\n",
      " - 55s - loss: -8.5713e-01 - val_loss: -7.8460e-01\n",
      "Epoch 14/20\n",
      " - 53s - loss: -8.6285e-01 - val_loss: -7.8168e-01\n",
      "Epoch 15/20\n",
      " - 56s - loss: -8.6802e-01 - val_loss: -7.7657e-01\n",
      "Epoch 16/20\n",
      " - 55s - loss: -8.7325e-01 - val_loss: -7.7450e-01\n",
      "Epoch 17/20\n",
      " - 57s - loss: -8.7829e-01 - val_loss: -7.7117e-01\n",
      "Epoch 18/20\n",
      " - 52s - loss: -8.8314e-01 - val_loss: -7.6574e-01\n",
      "Epoch 19/20\n",
      " - 53s - loss: -8.8726e-01 - val_loss: -7.6316e-01\n",
      "Epoch 20/20\n",
      " - 52s - loss: -8.9144e-01 - val_loss: -7.6215e-01\n",
      "Total Sequences: 758013\n",
      "Total Sequences: 70517\n",
      "Train on 7580 samples, validate on 704 samples\n",
      "Epoch 1/20\n",
      " - 39s - loss: -6.6705e-01 - val_loss: -7.0464e-01\n",
      "Epoch 2/20\n",
      " - 37s - loss: -7.0674e-01 - val_loss: -7.1728e-01\n",
      "Epoch 3/20\n",
      " - 37s - loss: -7.1526e-01 - val_loss: -7.2159e-01\n",
      "Epoch 4/20\n",
      " - 37s - loss: -7.2017e-01 - val_loss: -7.2357e-01\n",
      "Epoch 5/20\n",
      " - 37s - loss: -7.2388e-01 - val_loss: -7.2444e-01\n",
      "Epoch 6/20\n",
      " - 37s - loss: -7.2695e-01 - val_loss: -7.2486e-01\n",
      "Epoch 7/20\n",
      " - 37s - loss: -7.2965e-01 - val_loss: -7.2493e-01\n",
      "Epoch 8/20\n",
      " - 37s - loss: -7.3212e-01 - val_loss: -7.2463e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20\n",
      " - 37s - loss: -7.3435e-01 - val_loss: -7.2407e-01\n",
      "Epoch 10/20\n",
      " - 37s - loss: -7.3642e-01 - val_loss: -7.2338e-01\n",
      "Epoch 11/20\n",
      " - 37s - loss: -7.3835e-01 - val_loss: -7.2232e-01\n",
      "Epoch 12/20\n",
      " - 37s - loss: -7.4018e-01 - val_loss: -7.2136e-01\n",
      "Epoch 13/20\n",
      " - 37s - loss: -7.4197e-01 - val_loss: -7.2031e-01\n",
      "Epoch 14/20\n",
      " - 37s - loss: -7.4372e-01 - val_loss: -7.1907e-01\n",
      "Epoch 15/20\n",
      " - 37s - loss: -7.4545e-01 - val_loss: -7.1776e-01\n",
      "Epoch 16/20\n",
      " - 38s - loss: -7.4717e-01 - val_loss: -7.1644e-01\n",
      "Epoch 17/20\n",
      " - 38s - loss: -7.4884e-01 - val_loss: -7.1502e-01\n",
      "Epoch 18/20\n",
      " - 38s - loss: -7.5048e-01 - val_loss: -7.1370e-01\n",
      "Epoch 19/20\n",
      " - 37s - loss: -7.5215e-01 - val_loss: -7.1233e-01\n",
      "Epoch 20/20\n",
      " - 39s - loss: -7.5370e-01 - val_loss: -7.1123e-01\n",
      "Train on 7580 samples, validate on 704 samples\n",
      "Epoch 1/20\n",
      " - 44s - loss: -6.8199e-01 - val_loss: -7.1740e-01\n",
      "Epoch 2/20\n",
      " - 42s - loss: -7.1407e-01 - val_loss: -7.2539e-01\n",
      "Epoch 3/20\n",
      " - 40s - loss: -7.2220e-01 - val_loss: -7.2758e-01\n",
      "Epoch 4/20\n",
      " - 40s - loss: -7.2834e-01 - val_loss: -7.2824e-01\n",
      "Epoch 5/20\n",
      " - 39s - loss: -7.3389e-01 - val_loss: -7.2757e-01\n",
      "Epoch 6/20\n",
      " - 39s - loss: -7.3914e-01 - val_loss: -7.2639e-01\n",
      "Epoch 7/20\n",
      " - 40s - loss: -7.4430e-01 - val_loss: -7.2461e-01\n",
      "Epoch 8/20\n",
      " - 41s - loss: -7.4932e-01 - val_loss: -7.2241e-01\n",
      "Epoch 9/20\n",
      " - 42s - loss: -7.5434e-01 - val_loss: -7.1989e-01\n",
      "Epoch 10/20\n",
      " - 41s - loss: -7.5930e-01 - val_loss: -7.1744e-01\n",
      "Epoch 11/20\n",
      " - 42s - loss: -7.6430e-01 - val_loss: -7.1475e-01\n",
      "Epoch 12/20\n",
      " - 41s - loss: -7.6911e-01 - val_loss: -7.1113e-01\n",
      "Epoch 13/20\n",
      " - 42s - loss: -7.7410e-01 - val_loss: -7.0788e-01\n",
      "Epoch 14/20\n",
      " - 42s - loss: -7.7898e-01 - val_loss: -7.0434e-01\n",
      "Epoch 15/20\n",
      " - 40s - loss: -7.8381e-01 - val_loss: -6.9986e-01\n",
      "Epoch 16/20\n",
      " - 40s - loss: -7.8848e-01 - val_loss: -6.9484e-01\n",
      "Epoch 17/20\n",
      " - 40s - loss: -7.9301e-01 - val_loss: -6.9103e-01\n",
      "Epoch 18/20\n",
      " - 39s - loss: -7.9707e-01 - val_loss: -6.8810e-01\n",
      "Epoch 19/20\n",
      " - 40s - loss: -8.0115e-01 - val_loss: -6.8548e-01\n",
      "Epoch 20/20\n",
      " - 43s - loss: -8.0491e-01 - val_loss: -6.8072e-01\n",
      "Train on 7580 samples, validate on 704 samples\n",
      "Epoch 1/20\n",
      " - 55s - loss: -6.9025e-01 - val_loss: -7.2162e-01\n",
      "Epoch 2/20\n",
      " - 52s - loss: -7.1704e-01 - val_loss: -7.2699e-01\n",
      "Epoch 3/20\n",
      " - 54s - loss: -7.2674e-01 - val_loss: -7.2752e-01\n",
      "Epoch 4/20\n",
      " - 52s - loss: -7.3579e-01 - val_loss: -7.2577e-01\n",
      "Epoch 5/20\n",
      " - 51s - loss: -7.4555e-01 - val_loss: -7.2240e-01\n",
      "Epoch 6/20\n",
      " - 57s - loss: -7.5629e-01 - val_loss: -7.1800e-01\n",
      "Epoch 7/20\n",
      " - 53s - loss: -7.6747e-01 - val_loss: -7.1209e-01\n",
      "Epoch 8/20\n",
      " - 54s - loss: -7.7930e-01 - val_loss: -7.0459e-01\n",
      "Epoch 9/20\n",
      " - 56s - loss: -7.9138e-01 - val_loss: -6.9470e-01\n",
      "Epoch 10/20\n",
      " - 53s - loss: -8.0331e-01 - val_loss: -6.8956e-01\n",
      "Epoch 11/20\n",
      " - 53s - loss: -8.1487e-01 - val_loss: -6.8524e-01\n",
      "Epoch 12/20\n",
      " - 52s - loss: -8.2567e-01 - val_loss: -6.7537e-01\n",
      "Epoch 13/20\n",
      " - 53s - loss: -8.3507e-01 - val_loss: -6.6763e-01\n",
      "Epoch 14/20\n",
      " - 50s - loss: -8.4351e-01 - val_loss: -6.6704e-01\n",
      "Epoch 15/20\n",
      " - 50s - loss: -8.5093e-01 - val_loss: -6.6379e-01\n",
      "Epoch 16/20\n",
      " - 49s - loss: -8.5741e-01 - val_loss: -6.6095e-01\n",
      "Epoch 17/20\n",
      " - 49s - loss: -8.6358e-01 - val_loss: -6.5694e-01\n",
      "Epoch 18/20\n",
      " - 49s - loss: -8.6855e-01 - val_loss: -6.5536e-01\n",
      "Epoch 19/20\n",
      " - 49s - loss: -8.7287e-01 - val_loss: -6.5465e-01\n",
      "Epoch 20/20\n",
      " - 49s - loss: -8.7706e-01 - val_loss: -6.4875e-01\n"
     ]
    }
   ],
   "source": [
    "#测试units个数\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "for cluster in range(0, 6):\n",
    "    if cluster == 2: continue\n",
    "    records[cluster] = dict()\n",
    "    X_train, y_train = generate_sequences_3D(train_data[cluster], window_size, word_vectors, drop_ratio=0.99)\n",
    "    X_test, y_test = generate_sequences_3D(test_data[cluster], window_size, word_vectors, drop_ratio=0.99)\n",
    "    for n_units in [64, 128, 256]:\n",
    "        history, model = train_lstm_model_1(X_train, y_train, X_test, y_test, vector_size, n_units)\n",
    "        del model\n",
    "        records[cluster][n_units] = history\n",
    "\n",
    "results = dict()\n",
    "for cluster in range(0, 6):\n",
    "    if cluster == 2: continue\n",
    "    results[cluster] = dict()\n",
    "    for k, v in records[cluster].items():\n",
    "        results[cluster][k] = dict()\n",
    "        results[cluster][k]['loss'] = v.history['loss']\n",
    "        results[cluster][k]['val_loss'] = v.history['val_loss']\n",
    "np.save(path + 'lstm_word2vec_units.npy', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试vector_size\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "for cluster in range(0, 6):\n",
    "    if cluster == 2: continue\n",
    "    X_train, y_train = generate_sequences_3D(train_data[cluster], window_size, word_vectors, drop_ratio=0.99)\n",
    "    X_test, y_test = generate_sequences_3D(test_data[cluster], window_size, word_vectors, drop_ratio=0.99)\n",
    "    history, model = train_lstm_model(X_train, y_train, X_test, y_test, vector_size)\n",
    "    del model\n",
    "    records[vector_size][cluster] = history\n",
    "\n",
    "results = dict()\n",
    "for k, v in records[vector_size].items():\n",
    "    results[k] = dict()\n",
    "    results[k]['loss'] = v.history['loss']\n",
    "    results[k]['val_loss'] = v.history['val_loss']\n",
    "np.save(path + 'lstm_word2vec_dim_'+str(vector_size)+'.npy', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4fd5531d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot history\n",
    "plt.plot(records[128][3].history['loss'], label='train')\n",
    "plt.plot(records[128][3].history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_in = X_test[0]\n",
    "test_in = test_in.reshape(1, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-10 15:37:10,418 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 0.7194439768791199),\n",
       " ('To', 0.693936288356781),\n",
       " ('broke', 0.6766509413719177),\n",
       " ('of', 0.5830400586128235),\n",
       " ('And', 0.48103225231170654),\n",
       " ('a', 0.32743772864341736),\n",
       " ('up', 0.1653699278831482),\n",
       " ('and', 0.1354544460773468),\n",
       " ('tumbling', 0.06964272260665894),\n",
       " ('down', 0.06605370342731476)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(positive=test_pred, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 313572\n",
      "0.0006379585326953748 2.8239015023155993e-05\n"
     ]
    }
   ],
   "source": [
    "#准确率测试\n",
    "\n",
    "# 准确率，覆盖率测试\n",
    "from keras.models import load_model\n",
    "\n",
    "def generate_sequences_3D_eval(dict_data, window_size, word_vectors, drop_ratio=0):\n",
    "    # get sequences\n",
    "    sequences = load_sequence(dict_data, window_size)\n",
    "    if drop_ratio>0:\n",
    "        sequences, sequences_drop = train_test_split(sequences, test_size=drop_ratio, random_state=42)\n",
    "    X, y = sequences[:,:-1], sequences[:, -1]\n",
    "    X = item_to_vec(word_vectors, X, window_size-1)\n",
    "    return X, y\n",
    "\n",
    "cluster = 0\n",
    "window_size = 5\n",
    "\n",
    "model = model = load_model(path+'model_'+str(cluster)+'.h5')\n",
    "\n",
    "hit = 0\n",
    "precision = 0.0\n",
    "\n",
    "X_test, y_test = generate_sequences_3D_eval(test_data[cluster], window_size, word_vectors, drop_ratio=0.99)\n",
    "precision = len(y_test)\n",
    "\n",
    "for i in range(0, len(X_test)):\n",
    "    pred_vec = model.predict(X_test[i].reshape(1, window_size, vector_size))\n",
    "    pred_set = word_vectors.most_similar(positive=pred_vec, topn=1)\n",
    "    for pred_item, score in pred_set:\n",
    "        if pred_item == y_test[i]:\n",
    "            hit += 1\n",
    "\n",
    "print(hit/precision, hit/recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
